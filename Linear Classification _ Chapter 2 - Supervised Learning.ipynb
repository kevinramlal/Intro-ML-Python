{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Models for Classification \n",
    "\n",
    "Linear models are extensively used for classification. The formula looks similair to linear regression - except if the function is smaller than 0, we classify as -1, if larger than 0, classify as 1. \n",
    "\n",
    "For linear models for classification, the decision boundary is a linear function of the input. A binary classifier seperates two classes using a line/plane/or hyperplane. \n",
    "\n",
    "The two most common linear classifications include **Logistic Regression**, and **Linear support vector machines** (Linear SVM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:77: DeprecationWarning: Function make_blobs is deprecated; Please import make_blobs directly from scikit-learn\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "C:\\Users\\kevin\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "import pandas as pd\n",
    "import mglearn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "X, y = mglearn.datasets.make_forge()\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3))\n",
    "for model, ax in zip([LinearSVC(), LogisticRegression()], axes):\n",
    "    clf = model.fit(X, y)\n",
    "    mglearn.plots.plot_2d_separator(clf, X, fill=False, eps=0.5,\n",
    "    ax=ax, alpha=.7)\n",
    "    mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)\n",
    "    ax.set_title(\"{}\".format(clf.__class__.__name__))\n",
    "    ax.set_xlabel(\"Feature 0\")\n",
    "    ax.set_ylabel(\"Feature 1\")\n",
    "axes[0].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both models come up with similiar boundary, and both apply an L2 Regularization. There is a tradeoff parameter that determines the strength of the regularization called C. Higher C tries to fit the training data as best as possible, and will stress the importance of each individual data point being classfied correctly. Low values of C will cause the algorithm to adjust to the majority, and emphasize coefficient vector *w* being close to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---Using C = 1----\n",
      "Training set score: 0.955\n",
      "Test set score: 0.958\n",
      "\n",
      "---Using C = 100----\n",
      "Training set score: 0.972\n",
      "Test set score: 0.965\n",
      "\n",
      " Using L1 Regularization, C = 1\n",
      "Training set score: 0.960\n",
      "Test set score: 0.958\n",
      "\n",
      "---Using C = 100 and L1 Reg----\n",
      "Training set score: 0.986\n",
      "Test set score: 0.979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, stratify=cancer.target, random_state=42)\n",
    "\n",
    "print('\\n---Using C = 1----')\n",
    "logreg = LogisticRegression().fit(X_train, y_train)\n",
    "print(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))\n",
    "\n",
    "print('\\n---Using C = 100----')\n",
    "logreg100 = LogisticRegression(C=100).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.3f}\".format(logreg100.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg100.score(X_test, y_test)))\n",
    "\n",
    "print(\"\\n Using L1 Regularization, C = 1\")\n",
    "logreg = LogisticRegression(penalty = \"l1\").fit(X_train, y_train)\n",
    "print(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))\n",
    "\n",
    "print('\\n---Using C = 100 and L1 Reg----')\n",
    "logreg100 = LogisticRegression(C=100, penalty = \"l1\").fit(X_train, y_train)\n",
    "print(\"Training set score: {:.3f}\".format(logreg100.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg100.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Math Background for Logistic Regression\n",
    "\n",
    "This model arises from the desire to model the posterior probabilities of the K classes via linear functions in x, while ensuring that they sum to one and remain in [0.1]. \n",
    "\n",
    "<img src=./images/linear_class/log1.png/>\n",
    "\n",
    "This logistic function is also called the sigmoid function $$\\frac{1}{1 + e^{-t}}$$\n",
    "\n",
    "Logistic Regression models are usually fit by maximum likelihood, using the conditional likelihood G given X.\n",
    "\n",
    "<img src=./images/linear_class/log2.png/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Math Background for SVM\n",
    "\n",
    "SVM can be thought of as a generalization for decision boundaries for classification. In the linear case, this would be a line in 2D or a hyperplane in higher dimensions. SVMS also can take on non-linear decision boundaries. \n",
    "\n",
    "In the linear case, we find the hyperplane with the biggest margin between the training points for class 1  and -1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Multiclassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
