{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression (OLS, Ridge, Lasso, ElasticNet)\n",
    "\n",
    "#### Make a prediction using a linear function of the input features.\n",
    "Linear models for regression can be characterized as regression models for which the prediction is a hyperplane of n dimensions, where n is the number of features. (If one feature - then its a line)\n",
    "\n",
    "<img src=\"file/images/linear_reg/lr1.png\">\n",
    "\n",
    "To find the optimal $\\beta$ we use either the MSE or RSS (Residual Square Error) -> take the derivative.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/linear_reg/lr2.png\">\n",
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Important: There are assumptions of OLS below (To do & Review)\n",
    "\n",
    "1. The model is linear $y_i = x_i^{'}\\beta + e_i$\n",
    "\n",
    "2. The observations $(y_i,x_i)$ come from an iid random sample.\n",
    "\n",
    "3. Strict Exogenity, $E(e_i|x_j) = 0$ for all $i,j$. $E(e|X) = 0$.\n",
    "\n",
    "4. The Variables have finite second moments $E(y^{2}_{i}) < \\infty$\n",
    "\n",
    "5. The second moment matrix of $x_i, Q_{xx} = E(x_ix_k^{'})$ is invertible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import mglearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slope: 0.39391\n",
      "Intercept: -0.03180\n",
      "\n",
      "Training Set Score: 0.67009\n",
      "Test Set Score: 0.65934\n",
      "\n",
      "----Boston Data---\n",
      "\n",
      "Training Set Score: 0.9448313975211594\n",
      "Test Set Score: 0.7758378393351706\n"
     ]
    }
   ],
   "source": [
    "X,y = mglearn.datasets.make_wave(n_samples = 60)\n",
    "X_train, X_test, y_train,y_test = train_test_split(X,y,random_state = 42)\n",
    "lr = LinearRegression().fit(X_train,y_train)\n",
    "print(\"Slope: {:.5f}\".format(float(lr.coef_)))\n",
    "print(\"Intercept: {:.5f}\".format(float(lr.intercept_)))\n",
    "print(\"\\nTraining Set Score: {:.5f}\".format(float(lr.score(X_train,y_train))))\n",
    "print(\"Test Set Score: {:.5f}\".format(float(lr.score(X_test,y_test))))\n",
    "\n",
    "#Using Boston Data\n",
    "print(\"\\n----Boston Data---\")\n",
    "X,y = mglearn.datasets.load_extended_boston()\n",
    "X_train, X_test, y_train,y_test = train_test_split(X,y,random_state = 42)\n",
    "lr = LinearRegression().fit(X_train,y_train)\n",
    "# print(\"Slope: {}\".format(lr.coef_))\n",
    "# print(\"Intercept: {}\".format(lr.intercept_))\n",
    "print(\"\\nTraining Set Score: {}\".format(lr.score(X_train,y_train)))\n",
    "print(\"Test Set Score: {}\".format(lr.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since training score is similiar to test score - we are likely underfitting. In contrast, if we were to have a high training score but a low test score - we would be overfitting. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge Regression\n",
    "\n",
    "\n",
    "Essentially Ridge Regression adds a penalty term to our cost function (MSE/RSS in the case of OLS). This penalty is also called \"L2 Regularization\". Effectively this shrinks the regression coefficients by imposing a penalty on their size. The Ridge coefficients minimize a penalized residual sum of squares, however, the intercept term is not included in the penalization. \n",
    "\n",
    "Note - Ridge Regression becomes a biased estimator.\n",
    "\n",
    "<img src=\"./images/linear_reg/lr3.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/linear_reg/lr4.png\">\n",
    "\n",
    "Normally, the inputs are standardized before solving. SVD* (Singular Value Decomposition) is used to solve for Ridge Regression  coef - review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Results using alpha = 1 (default)---\n",
      "\n",
      "Training Set Score: 0.8700969775259918\n",
      "Test Set Score: 0.8145421818415552\n",
      "\n",
      "--Results using alpha = 10---\n",
      "\n",
      "Training Set Score: 0.7669532344928074\n",
      "Test Set Score: 0.7279392273706594\n",
      "\n",
      "--Results using alpha = .10---\n",
      "\n",
      "Training Set Score: 0.9174870139232201\n",
      "Test Set Score: 0.8246491875314618\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge().fit(X_train,y_train)\n",
    "print(\"--Results using alpha = 1 (default)---\")\n",
    "# print(\"\\nSlope: {}\".format(ridge.coef_))\n",
    "# print(\"Intercept: {}\".format(ridge.intercept_))\n",
    "print(\"\\nTraining Set Score: {}\".format(ridge.score(X_train,y_train)))\n",
    "print(\"Test Set Score: {}\".format(ridge.score(X_test,y_test)))\n",
    "\n",
    "\n",
    "ridge = Ridge(alpha = 10).fit(X_train,y_train)\n",
    "print(\"\\n--Results using alpha = 10---\")\n",
    "# print(\"\\nSlope: {}\".format(ridge.coef_))\n",
    "# print(\"Intercept: {}\".format(ridge.intercept_))\n",
    "print(\"\\nTraining Set Score: {}\".format(ridge.score(X_train,y_train)))\n",
    "print(\"Test Set Score: {}\".format(ridge.score(X_test,y_test)))\n",
    "\n",
    "ridge = Ridge(alpha = 0.1).fit(X_train,y_train)\n",
    "print(\"\\n--Results using alpha = .10---\")\n",
    "# print(\"\\nSlope: {}\".format(ridge.coef_))\n",
    "# print(\"Intercept: {}\".format(ridge.intercept_))\n",
    "print(\"\\nTraining Set Score: {}\".format(ridge.score(X_train,y_train)))\n",
    "print(\"Test Set Score: {}\".format(ridge.score(X_test,y_test)))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression\n",
    "\n",
    "Similiarly to Ridge Regression, lasso regression restricts coefficients to be close to zero, but by using \"L1 regularization\". The consequence is that some coefficients become exactly zero, meaning this method can be thought of as a \"feature selection\" type method. \n",
    "<img src=\"./images/linear_reg/lr5.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Using Alpha = 1 (default)---\n",
      "Training set score: 0.27\n",
      "Test set score: 0.26\n",
      "Number of features used: 3\n",
      "\n",
      "----Using Alpha = 10---\n",
      "Training set score: 0.00\n",
      "Test set score: -0.03\n",
      "Number of features used: 0\n",
      "\n",
      "----Using Alpha = .10---\n",
      "Training set score: 0.75\n",
      "Test set score: 0.70\n",
      "Number of features used: 12\n",
      "\n",
      "----Using Alpha = .0010---\n",
      "Training set score: 0.93\n",
      "Test set score: 0.81\n",
      "Number of features used: 76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso().fit(X_train, y_train)\n",
    "print(\"----Using Alpha = 1 (default)---\")\n",
    "print(\"Training set score: {:.2f}\".format(lasso.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(lasso.score(X_test, y_test)))\n",
    "print(\"Number of features used: {}\".format(np.sum(lasso.coef_ != 0)))\n",
    "\n",
    "lasso = Lasso(alpha = 10).fit(X_train, y_train) #In the result we see using this basically eliminates all features\n",
    "print(\"\\n----Using Alpha = 10---\")\n",
    "print(\"Training set score: {:.2f}\".format(lasso.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(lasso.score(X_test, y_test)))\n",
    "print(\"Number of features used: {}\".format(np.sum(lasso.coef_ != 0)))\n",
    "\n",
    "lasso = Lasso(alpha = .10).fit(X_train, y_train)\n",
    "print(\"\\n----Using Alpha = .10---\")\n",
    "print(\"Training set score: {:.2f}\".format(lasso.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(lasso.score(X_test, y_test)))\n",
    "print(\"Number of features used: {}\".format(np.sum(lasso.coef_ != 0))) \n",
    "\n",
    "lasso = Lasso(alpha = .0010).fit(X_train, y_train) #using too small of an alpha causes convergence issues (see warning)\n",
    "print(\"\\n----Using Alpha = .0010---\") \n",
    "print(\"Training set score: {:.2f}\".format(lasso.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(lasso.score(X_test, y_test)))\n",
    "print(\"Number of features used: {}\".format(np.sum(lasso.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly there is \"Elastic Net\" which combines the penalties for both Ridge and Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Using Alpha = 1 (default)---\n",
      "Training set score: 0.29\n",
      "Test set score: 0.30\n",
      "Number of features used: 34\n",
      "\n",
      "----Using Alpha = 10 ---\n",
      "Training set score: 0.00\n",
      "Test set score: -0.03\n",
      "Number of features used: 0\n",
      "\n",
      "----Using Alpha = 0.01 (default)---\n",
      "Training set score: 0.84\n",
      "Test set score: 0.79\n",
      "Number of features used: 82\n",
      "\n",
      "----Using Alpha = 0.00001 (default)---\n",
      "Training set score: 0.94\n",
      "Test set score: 0.78\n",
      "Number of features used: 104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:492: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "en = ElasticNet().fit(X_train, y_train) #doesnt do very well \n",
    "print(\"----Using Alpha = 1 (default)---\")\n",
    "print(\"Training set score: {:.2f}\".format(en.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(en.score(X_test, y_test)))\n",
    "print(\"Number of features used: {}\".format(np.sum(en.coef_ != 0)))\n",
    "\n",
    "en = ElasticNet(alpha = 10).fit(X_train, y_train) #removes all features\n",
    "print(\"\\n----Using Alpha = 10 ---\")\n",
    "print(\"Training set score: {:.2f}\".format(en.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(en.score(X_test, y_test)))\n",
    "print(\"Number of features used: {}\".format(np.sum(en.coef_ != 0)))\n",
    "\n",
    "en = ElasticNet(alpha = 0.01).fit(X_train, y_train) #solid results \n",
    "print(\"\\n----Using Alpha = 0.01 (default)---\")\n",
    "print(\"Training set score: {:.2f}\".format(en.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(en.score(X_test, y_test)))\n",
    "print(\"Number of features used: {}\".format(np.sum(en.coef_ != 0)))\n",
    "\n",
    "en = ElasticNet(alpha = 0.00001).fit(X_train, y_train) #overfitting\n",
    "print(\"\\n----Using Alpha = 0.00001 (default)---\")\n",
    "print(\"Training set score: {:.2f}\".format(en.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(en.score(X_test, y_test)))\n",
    "print(\"Number of features used: {}\".format(np.sum(en.coef_ != 0)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
